{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHROMA Inference Demo\n",
    "\n",
    "This notebook demonstrates how to load a pre-trained/fine-tuned CHROMA model and perform inference on a single chromosome image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Import CHROMA model definitions\n",
    "import models_vit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "Set up the model parameters and paths. Ensure `checkpoint_path` points to your fine-tuned weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    model = 'vit_large_patch16'\n",
    "    input_size = 224\n",
    "    nb_classes = 24  # Change to 2 or 5 depending on your task\n",
    "    drop_path = 0.0\n",
    "    global_pool = 'avg'\n",
    "    checkpoint_path = './finetune_chr_24_class/checkpoint-best.pth' # Path to your weight file\n",
    "    image_path = './test_images/sample_chr.jpg' # Path to a test image\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model\n",
    "Initialize the ViT model and load the state dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(args):\n",
    "    print(f\"Creating model: {args.model}\")\n",
    "    model = models_vit.__dict__[args.model](\n",
    "        img_size=args.input_size,\n",
    "        num_classes=args.nb_classes,\n",
    "        drop_path_rate=args.drop_path,\n",
    "        global_pool=args.global_pool,\n",
    "    )\n",
    "\n",
    "    if os.path.exists(args.checkpoint_path):\n",
    "        print(f\"Loading checkpoint: {args.checkpoint_path}\")\n",
    "        checkpoint = torch.load(args.checkpoint_path, map_location='cpu')\n",
    "        checkpoint_model = checkpoint['model']\n",
    "        \n",
    "        # Interpolate position embeddings if necessary\n",
    "        from util.pos_embed import interpolate_pos_embed\n",
    "        interpolate_pos_embed(model, checkpoint_model)\n",
    "        \n",
    "        # Load weights\n",
    "        msg = model.load_state_dict(checkpoint_model, strict=False)\n",
    "        print(\"Model loaded successfully.\")\n",
    "        print(f\"Missing keys (if any): {msg.missing_keys}\")\n",
    "    else:\n",
    "        print(\"Warning: Checkpoint not found! Using random initialization.\")\n",
    "\n",
    "    model.to(args.device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model = load_model(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess Image\n",
    "Define the standard transform for inference (Resize -> CenterCrop -> Normalize)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=3),\n",
    "    transforms.CenterCrop(args.input_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def load_image(image_path):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    input_tensor = transform(img).unsqueeze(0) # Add batch dimension\n",
    "    return img, input_tensor\n",
    "\n",
    "# Create a dummy image if file doesn't exist for demo purposes\n",
    "if not os.path.exists(args.image_path):\n",
    "    print(\"Test image not found, creating a dummy image...\")\n",
    "    os.makedirs(os.path.dirname(args.image_path), exist_ok=True)\n",
    "    Image.new('RGB', (300, 300), color='gray').save(args.image_path)\n",
    "\n",
    "original_img, input_tensor = load_image(args.image_path)\n",
    "input_tensor = input_tensor.to(args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(input_tensor)\n",
    "    probs = torch.nn.Softmax(dim=1)(outputs)\n",
    "    conf, pred_class = torch.max(probs, 1)\n",
    "\n",
    "print(f\"Predicted Class Index: {pred_class.item()}\")\n",
    "print(f\"Confidence: {conf.item():.4f}\")\n",
    "\n",
    "# Visualization\n",
    "plt.imshow(original_img)\n",
    "plt.title(f\"Prediction: Class {pred_class.item()} ({conf.item():.2%})\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}{
 "cells": [],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
